{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## SOURCE CODE AND NOTEBOOK -TINGYU  -IEL MEMBER (PROJECT TEAM MEMBER)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib inline\n",
    "import missingno as msno\n",
    "np.random.seed(19)\n",
    "from matplotlib.pyplot import figure\n",
    "data_directory = '/work2/05067/nagyz/austin_energy/data/'\n",
    "filepaths = [os.path.join(data_directory,f) for f in os.listdir(data_directory) if f.endswith('.csv')]\n",
    "from datetime import datetime \n",
    "from Graphs import load_data,load_graph_kwh,yearly_graph_kwh,load_graph_count,monthly_graph_kwh,daily_graph_kwh\n",
    "from Graphs import load_graph_kwh_and_count,yearly_graph_kwh_and_count,monthly_graph_kwh_and_count,daily_graph_kwh_and_count\n",
    "from agg_and_heatmap_functions import length,prav3,prav2,prav1,dinostorm,plot_aggraph,heatmap,finalheatmap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, SGDRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor,GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time\n",
       "2017-01-01 00:00:00+00:00    17.0\n",
       "2017-01-02 00:00:00+00:00    17.4\n",
       "2017-01-03 00:00:00+00:00    15.2\n",
       "2017-01-04 00:00:00+00:00     5.9\n",
       "2017-01-05 00:00:00+00:00     8.6\n",
       "                             ... \n",
       "2021-02-25 00:00:00+00:00    13.8\n",
       "2021-02-26 00:00:00+00:00    12.0\n",
       "2021-02-27 00:00:00+00:00    19.5\n",
       "2021-02-28 00:00:00+00:00    21.7\n",
       "2021-03-01 00:00:00+00:00    11.7\n",
       "Name: tavg, Length: 1521, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = './data/'\n",
    "export_dir = os.path.join(data_dir, '_4_exports_UT')\n",
    "id_list = pd.read_csv('temporal_percentage1.csv')\n",
    "id_list.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "weather_daily=pd.read_csv('Austin_weather_daily.csv')\n",
    "weather_daily['time']=pd.to_datetime(weather_daily['time'],utc=True)\n",
    "weather_daily1=weather_daily.set_index('time',drop=True)\n",
    "weather_austin_daily=weather_daily1['tavg']\n",
    "w1=weather_austin_daily['2017-01-01 00:00:00+00:00':'2021-03-01 00:00:00+00:00']\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list['buildingType'].unique()\n",
    "res_type = ['SINGLE FAMILY', 'MULTIFAMILY',  'FOURPLEX', \n",
    "            'DUPLEX', 'CONDOS', 'CONDO (STACKED)', \n",
    "            '1 FAM DWELLING, ACCESSORY DWELLING UNIT',\n",
    "            'TOWNHOMES','MOHO SINGLE PP', \n",
    "            '1 FAM DWELLING, GARAGE APARTMENT', 'MOHO DOUBLE PP',\n",
    "            'MOHO SINGLE REAL', 'MOHO DOUBLE REAL', 'TRIPLEX', \n",
    "            '1 FAM DWELLING, MOHO DOUBLE REAL',\n",
    "           '1 FAM DWELLING, MOHO SINGLE REAL']\n",
    "\n",
    "commercial_type = ['SM OFFICE CONDO', 'RESTAURANT',\n",
    "       'OFFICE MED 10-35', 'OFFICE LG >35000', 'OFFICE (SMALL)',\n",
    "       'LG OFFICE CONDO', 'REGIONAL SHOP CT', 'FAST FOOD REST',\n",
    "       'SM STORE <10K SF','CONVENIENCE STOR',\n",
    "       'COMMERCIAL IMPROVED', 'COMMERCIAL SPACE CONDOS',\n",
    "       'OFF HI-RISE >= 6']\n",
    "\n",
    "other_type = [x for x in id_list['buildingType'].unique() if x not in res_type and x not in  commercial_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_winterstorm_data(point):\n",
    "    example_ts=pd.read_csv('/work2/05067/nagyz/austin_energy/data/_4_exports_UT/UT_sID_'+str(point)+'_dates_2017.01.01_through_2021.10.01.csv')\n",
    "    example_ts.index = pd.to_datetime(example_ts.index)\n",
    "    start_time='2017-01-01 00:00:00+00:00'\n",
    "    end_time='2021-03-01 00:00:00+00:00'\n",
    "    example_ts=example_ts[start_time:end_time]\n",
    "    df = pd.DataFrame()\n",
    "    for year in range(2017, 2020):\n",
    "        year = str(year)\n",
    "        row_i_1to3 = w1[f'{year}-01':f'{year}-03'].argmin()\n",
    "        month_1to3 = w1.index[row_i_1to3].month\n",
    "        index_string1 = f'{year}-{month_1to3}'\n",
    "        df = pd.concat([df, example_ts[index_string1:index_string1]])\n",
    "        \n",
    "        row_i_4to6 = w1[f'{year}-04':f'{year}-06'].argmin()\n",
    "        month_4to6 = w1.index[row_i_4to6].month\n",
    "        index_string2 = f'{year}-{month_4to6}'\n",
    "        df = pd.concat([df, example_ts[index_string2:index_string2]])\n",
    "        \n",
    "        row_i_7to9 = w1[f'{year}-07':f'{year}-09'].argmin()\n",
    "        month_7to9 = w1.index[row_i_7to9].month\n",
    "        index_string3 = f'{year}-{month_7to9}'\n",
    "        df = pd.concat([df, example_ts[index_string3:index_string3]])\n",
    "        \n",
    "        row_i_10to12 = w1[f'{year}-10':f'{year}-12'].argmin()\n",
    "        month_10to12 = w1.index[row_i_10to12].month\n",
    "        index_string4 = f'{year}-{month_10to12}'\n",
    "        df = pd.concat([df, example_ts[index_string4:index_string4]])\n",
    "        \n",
    "        \n",
    "    df = pd.concat([df, example_ts['2021-01-01':'2021-01-31']])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2017-01-04 00:00:00+00:00', '2017-01-05 00:00:00+00:00',\n",
       "               '2017-01-06 00:00:00+00:00', '2017-01-07 00:00:00+00:00',\n",
       "               '2017-01-08 00:00:00+00:00', '2017-01-09 00:00:00+00:00',\n",
       "               '2017-01-14 00:00:00+00:00', '2017-01-17 00:00:00+00:00',\n",
       "               '2017-01-18 00:00:00+00:00', '2017-01-19 00:00:00+00:00',\n",
       "               ...\n",
       "               '2021-02-19 00:00:00+00:00', '2021-02-20 00:00:00+00:00',\n",
       "               '2021-02-21 00:00:00+00:00', '2021-02-22 00:00:00+00:00',\n",
       "               '2021-02-23 00:00:00+00:00', '2021-02-24 00:00:00+00:00',\n",
       "               '2021-02-25 00:00:00+00:00', '2021-02-26 00:00:00+00:00',\n",
       "               '2021-02-27 00:00:00+00:00', '2021-02-28 00:00:00+00:00'],\n",
       "              dtype='datetime64[ns, UTC]', name='time', length=392, freq=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_daily=pd.read_csv('Austin_weather_daily.csv')\n",
    "weather_daily['time']=pd.to_datetime(weather_daily['time'],utc=True)\n",
    "weather_daily1=weather_daily.set_index('time',drop=True)\n",
    "weather_austin_daily=weather_daily1['tavg']\n",
    "cool_points=weather_austin_daily[weather_austin_daily<15]['2017-01-01 00:00:00+00:00':'2021-01-31 00:00:00+00:00']\n",
    "winter_index=weather_austin_daily['2021-01-31 00:00:00+00:00':'2021-02-28 00:00:00+00:00'].index\n",
    "s=pd.Series(index = cool_points.index)+pd.Series(index = winter_index)\n",
    "s.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [4:40:55<00:00, 110.16s/it]  \n"
     ]
    }
   ],
   "source": [
    "freq, freq_long_name ='D', 'daily'\n",
    "check_timedelta = pd.Timedelta(1, freq)\n",
    "\n",
    "        # residential\n",
    "res_type_id_list = id_list[id_list['buildingType'].isin(res_type)].copy()\n",
    "res_type_id_list['weight'] = res_type_id_list['squareFt'] / res_type_id_list['squareFt'].sum()\n",
    "daily_date_range = pd.date_range('20210301', '20210731', freq=freq, tz = 'UTC')\n",
    "res_type_id_list_ts = pd.Series(index = daily_date_range)\n",
    "\n",
    "for day in tqdm(daily_date_range):\n",
    "    energy_data = pd.DataFrame(columns = ['squareFt', 'energyUsage'])\n",
    "    for i, row in res_type_id_list.iterrows():\n",
    "        square_ft = row['squareFt']\n",
    "        point = int(row['Service Point'])\n",
    "\n",
    "        example_ts=pd.read_csv('/work2/05067/nagyz/austin_energy/data/_4_exports_UT/UT_sID_'+str(point)+'_dates_2017.01.01_through_2021.10.01.csv')\n",
    "        example_ts.index = pd.to_datetime(example_ts.index)\n",
    "        daily_data = example_ts[day:day + check_timedelta][:-1]\n",
    "        energy_data.loc[point] = [square_ft, daily_data.iloc[:, 0].mean()]\n",
    "\n",
    "    weigheted_data = (energy_data['energyUsage'] * energy_data['squareFt'] / energy_data['squareFt'].sum()).sum()\n",
    "    res_type_id_list_ts[day] = weigheted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021-03-01 00:00:00+00:00    1181.713679\n",
       "2021-03-02 00:00:00+00:00    1199.621811\n",
       "2021-03-03 00:00:00+00:00    1160.971100\n",
       "2021-03-04 00:00:00+00:00    1043.311550\n",
       "2021-03-05 00:00:00+00:00    1006.897375\n",
       "                                ...     \n",
       "2021-07-27 00:00:00+00:00            NaN\n",
       "2021-07-28 00:00:00+00:00            NaN\n",
       "2021-07-29 00:00:00+00:00            NaN\n",
       "2021-07-30 00:00:00+00:00            NaN\n",
       "2021-07-31 00:00:00+00:00            NaN\n",
       "Freq: D, Length: 153, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_type_id_list_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_type_id_list_ts.to_csv('resi_covid_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [29:02<00:00, 29.05s/it]\n"
     ]
    }
   ],
   "source": [
    "freq, freq_long_name ='D', 'daily'\n",
    "check_timedelta = pd.Timedelta(1, freq)\n",
    "check_qa_len = len(pd.date_range(pd.Timestamp('2017-01-01T12'), pd.Timestamp('2017-01-01T12') + check_timedelta, freq = '15min')[:-1]) / 2\n",
    "    \n",
    "    # residential\n",
    "other_type_id_list = id_list[id_list['buildingType'].isin(other_type)].copy()\n",
    "other_type_id_list['weight'] = other_type_id_list['squareFt'] / other_type_id_list['squareFt'].sum()\n",
    "daily_date_range = pd.date_range('20210101', '20210301', freq=freq, tz = 'UTC')\n",
    "other_type_id_list_ts = pd.Series(index = daily_date_range)\n",
    "for day in tqdm(daily_date_range):\n",
    "    energy_data = pd.DataFrame(columns = ['squareFt', 'Count'])\n",
    "    for i, row in other_type_id_list.iterrows():\n",
    "        square_ft = row['squareFt']\n",
    "        point = int(row['Service Point'])\n",
    "            \n",
    "        example_ts=pd.read_csv('/work2/05067/nagyz/austin_energy/data/_4_exports_UT/UT_sID_'+str(point)+'_dates_2017.01.01_through_2021.10.01.csv')\n",
    "        example_ts.index = pd.to_datetime(example_ts.index)\n",
    "        daily_data = example_ts[day:day + check_timedelta][:-1]\n",
    "        if len(daily_data) > check_qa_len: # if the value contains like 50% below, then we do not count for that\n",
    "            energy_data.loc[point] = [square_ft, daily_data.iloc[:, 2].mean()]\n",
    "            \n",
    "    weigheted_data = (energy_data['Count'] * energy_data['squareFt'] / energy_data['squareFt'].sum()).sum()\n",
    "    #weigheted_count= (energy_data['count'] * energy_data['squareFt'] / energy_data['squareFt'].sum()).sum()\n",
    "    other_type_id_list_ts[day] = weigheted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_type_id_list_ts.to_csv('other_2021_part.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_line=make_pipeline(RandomForestRegressor())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
